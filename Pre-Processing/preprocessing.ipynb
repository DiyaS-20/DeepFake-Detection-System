{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a21410",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import cv2 as cv\n",
    "import glob\n",
    "train_sample_metadata = pd.read_json('/content/drive/MyDrive/Dataset/train_sample_videos/metadata.json').T\n",
    "train_sample_metadata.head()\n",
    "train_sample_metadata.info()\n",
    "train_sample_metadata['label'].value_counts()\n",
    "train_sample_metadata.groupby('label')['label'].count().plot(figsize=(10, 5), kind='bar', title='Distribution of Labels in the Training Set')\n",
    "plt.show()\n",
    "def unique_values(data):\n",
    "    total = data.count()\n",
    "    tt = pd.DataFrame(total)\n",
    "    tt.columns = ['Total']\n",
    "    uniques = []\n",
    "    for col in data.columns:\n",
    "        unique = data[col].nunique()\n",
    "        uniques.append(unique)\n",
    "    tt['Uniques'] = uniques\n",
    "    return(np.transpose(tt))\n",
    "def most_frequent_values(data):\n",
    "    total = data.count()\n",
    "    tt = pd.DataFrame(total)\n",
    "    tt.columns = ['Total']\n",
    "    items = []\n",
    "    vals = []\n",
    "    for col in data.columns:\n",
    "        itm = data[col].value_counts().index[0]\n",
    "        val = data[col].value_counts().values[0]\n",
    "        items.append(itm)\n",
    "        vals.append(val)\n",
    "    tt['Most frequent item'] = items\n",
    "    tt['Frequency'] = vals\n",
    "    tt['Percent from total'] = np.round(vals / total * 100, 3)\n",
    "    return(np.transpose(tt))\n",
    "most_frequent_values(train_sample_metadata)\n",
    "original_counts = pd.DataFrame(train_sample_metadata['original'].value_counts())\n",
    "original_counts.head(10)\n",
    "fake_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label=='FAKE'].sample(10).index)\n",
    "fake_train_sample_video\n",
    "real_train_sample_video = list(train_sample_metadata.loc[train_sample_metadata.label=='REAL'].sample(10).index)    # returning the index value which is video name\n",
    "real_train_sample_video\n",
    "def display_image_from_video(video_path):\n",
    "    '''\n",
    "    input: video_path - path for video\n",
    "    process:\n",
    "    1. perform a video capture from the video\n",
    "    2. read the image\n",
    "    3. display the image\n",
    "    '''\n",
    "    capture_image = cv.VideoCapture(video_path) \n",
    "    ret, frame = capture_image.read()\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)   # converting the frame color to RGB\n",
    "    ax.imshow(frame)\n",
    "for video in fake_train_sample_video:\n",
    "  display_image_from_video(os.path.join(\"/content/drive/MyDrive/Dataset/train_sample_videos/\"+video))\n",
    "  print()\n",
    "  print()\n",
    "for video in real_train_sample_video:\n",
    "  display_image_from_video(os.path.join(\"/content/drive/MyDrive/Dataset/train_sample_videos/\"+video))\n",
    "  print()\n",
    "  print()\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    " \n",
    "def play_video(video_file):\n",
    "    '''\n",
    "    Display video\n",
    "    param: video_file - the name of the video file to display\n",
    "    param: subset - the folder where the video file is located (can be TRAIN_SAMPLE_FOLDER or TEST_Folder)\n",
    "    '''\n",
    "    video_url = open(video_file,'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(video_url).decode()\n",
    "    return HTML(\"\"\"<video width=500 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)\n",
    "videos = glob.glob('/content/drive/MyDrive/Dataset/train_sample_videos/*.mp4')\n",
    "frame_cnt = []\n",
    "for video in videos:\n",
    "  capture = cv.VideoCapture(video)\n",
    "  frame_cnt.append(int(capture.get(cv.CAP_PROP_FRAME_COUNT)))\n",
    "print(\"Frames: \",frame_cnt)\n",
    "print(\"Avg Frame per video: \",np.mean(frame_cnt))\n",
    "!pip install face_recognition\n",
    "def image_from_video(video_path):\n",
    "    '''\n",
    "    input: video_path - path for video\n",
    "    process:\n",
    "    1. perform a video capture from the video\n",
    "    2. read the image\n",
    "    3. display the image\n",
    "    '''\n",
    "    capture_image = cv.VideoCapture(video_path) \n",
    "    ret, frame = capture_image.read()\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)   # converting the frame color to RGB\n",
    "    ax.imshow(frame)\n",
    " \n",
    "    return frame\n",
    "image = image_from_video(\"/content/drive/MyDrive/Dataset/train_sample_videos/aagfhgtpmv.mp4\")\n",
    " \n",
    "import face_recognition\n",
    "face_locations = face_recognition.face_locations(image)\n",
    " \n",
    "from PIL import Image\n",
    " \n",
    " \n",
    "for face_location in face_locations:\n",
    " \n",
    "  top,right,bottom,left = face_location\n",
    "  print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n",
    "  face_image = image[top:bottom, left:right]\n",
    "  fig, ax = plt.subplots(1,1, figsize=(5, 5))\n",
    "  plt.grid(False)\n",
    "  ax.xaxis.set_visible(False)\n",
    "  ax.yaxis.set_visible(False)\n",
    "  ax.imshow(face_image)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
